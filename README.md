# Azure-Data-factory-Repo

#AZURE DEVOPS REPO CONNECTED TO AZURE AND DATABRICKS CHECK THE BRANCHES 

Responsiblities and learnings from this project
  Having good experience in creating pipelines for data ingestion with metadata-driven pipelines  
• Created metadata tables as per the business requirements to connection details and metadata  
• Involved in making connectivity between Azure Databricks and Azure Data Lake Storage Gen2 with SPN  
• Extensively used Azure Key Vaults to mask credentials and used them while connecting to the Data Sources  
• Having been involved in configuring Integration Runtimes as per requirement  
• Having good experience in utilizing log tables to feed the log details while executing data ingestion pipelines  
• Created parameterized linked services  
• Created parameterized datasets  
• Extensively used Lookup Activity, Foreach Activity, Get Metadata Activity, Copy Data Activity, Notebook Activity, Web 
Activity  
• Implemented e-mail alters when the ADF pipeline is failed or succeeded with the help of Logic App  
• Having good experience in incorporating Azure DevOps Git in Data Factory  
• Having good knowledge of moving code from a lower environment to a higher environment by creating Azure DevOps 
pipelines  
• Having good experience in writing notebooks by using Pyspark, SparkSQL  
• Used widgets in the Notebook to get values from Azure Data Factory  
• Created triggers in Azure Data Factory to schedule Pipelines  
• Having good experience in writing Stored procedures and complex queries  
• Involved in end-to-end processing with Azure Data Factory and Azure Databricks  
• Having good experience creating incremental loading pipelines  

Key accomplishments and skills include:

Establishing connectivity between Azure Databricks and Azure Data Lake Storage Gen2 using Service Principal Names (SPN).
Utilizing Azure Key Vaults to securely mask credentials for data source connections.
Configuring Integration Runtimes tailored to specific requirements.
Leveraging log tables to record details during the execution of data ingestion pipelines.
Developing parameterized linked services and datasets for flexible pipeline configurations.
Proficient use of various Azure Data Factory activities such as Lookup, Foreach, Get Metadata, Copy Data, Notebook, and Web.
Implementing email alerts for pipeline success or failure using Logic Apps.
Integrating Azure DevOps Git with Azure Data Factory for version control and collaboration.
Moving code across environments by creating and managing Azure DevOps pipelines.
Writing and optimizing notebooks in PySpark and SparkSQL, including the use of widgets to receive values from Azure Data Factory.
Scheduling pipelines using triggers in Azure Data Factory.
Writing complex stored procedures and queries.
Managing end-to-end data processing with Azure Data Factory and Azure Databricks.
Creating pipelines for incremental data loading.
